
"""
scanner.py ‚Äî Silnik EVT / Barbell Convexity Scanner v2.0

Matematyczne fundamenty (bez AI API):
  ‚Ä¢ Extreme Value Theory: GPD-POT dla prawego ORAZ lewego ogona (Pickands 1975, Balkema-de Haan 1974)
  ‚Ä¢ Hurst Exponent R/S Analysis (Peters 1994, Lo 1991) ‚Äî trend vs. mean-reversion
  ‚Ä¢ Amihud (2002) Illiquidity Ratio ‚Äî rzeczywista p≈Çynno≈õƒá transakcyjna
  ‚Ä¢ Omega Ratio (Shadwick & Keating 2002) ‚Äî nie zak≈Çada normalno≈õci rozk≈Çadu
  ‚Ä¢ Momentum 12-1 (Jegadeesh & Titman 1993) ‚Äî czynnik trendu cenowego
  ‚Ä¢ Composite Barbell Score ‚Äî wa≈ºony Z-Score 7 czynnik√≥w (wzorowany AQR, Asness et al. 2012)
  ‚Ä¢ MST Mantegna (1999) + HRP Dendrogram (Lopez de Prado 2016)
"""

import numpy as np
import pandas as pd
from scipy.stats import skew, kurtosis, genpareto
import scipy.cluster.hierarchy as sch
import scipy.spatial.distance as ssd
import streamlit as st
import plotly.graph_objects as go
import plotly.figure_factory as ff
from modules.logger import setup_logger

logger = setup_logger(__name__)

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

from modules.metrics import calculate_sharpe, calculate_sortino, calculate_max_drawdown


# ‚îÄ‚îÄ‚îÄ EVT ‚Äî Prawy Ogon (Zyski / Asymetryczne Wzrosty) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def evt_pot_right_tail(returns: np.ndarray, threshold_quantile: float = 0.90) -> float:
    """
    GPD Peaks-Over-Threshold ‚Äî prawy ogon (zyski).
    xi > 0 = grube ogony zysku = wypuk≈Çe aktywo (idealne do Risky Sleeve).
    Odwo≈Çanie: Pickands (1975), Balkema & de Haan (1974).
    """
    if len(returns) < 50:
        return np.nan
    positive = returns[returns > 0]
    if len(positive) < 20:
        return np.nan
    u = np.quantile(positive, threshold_quantile)
    exc = positive[positive > u] - u
    if len(exc) < 5:
        return np.nan
    try:
        xi, _, _ = genpareto.fit(exc, floc=0)
        return float(xi)
    except Exception as e:
        logger.debug(f"EVT Right Tail fit failed: {e}")
        return np.nan


def evt_pot_left_tail(returns: np.ndarray, threshold_quantile: float = 0.05) -> float:
    """
    GPD Peaks-Over-Threshold ‚Äî lewy ogon (straty / Crash Risk).
    xi > 0 = grube ogony STRATY = niebezpieczne aktywo do Barbella.
    Wysoki xi_left ‚Üí dyskwalifikacja z Risky Sleeve.
    Odwo≈Çanie: Adrian & Brunnermeier (2011, CoVaR), Acharya et al. (2012, MES).
    """
    if len(returns) < 50:
        return np.nan
    losses = -returns[returns < 0]
    if len(losses) < 20:
        return np.nan
    u = np.quantile(losses, 1 - threshold_quantile)
    exc = losses[losses > u] - u
    if len(exc) < 5:
        return np.nan
    try:
        xi, _, _ = genpareto.fit(exc, floc=0)
        return float(xi)
    except Exception as e:
        logger.debug(f"EVT Left Tail fit failed: {e}")
        return np.nan


# ‚îÄ‚îÄ‚îÄ Wyk≈Çadnik Hursta ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def calculate_hurst_exponent(price_series: pd.Series, min_lag: int = 10, max_lag: int = 100) -> float:
    """
    R/S Analysis ‚Äî Wyk≈Çadnik Hursta.
    H > 0.55: trendowanie (persistent) ‚Äî idealne dla strategii Momentum.
    H = 0.50: b≈ÇƒÖdzenie losowe ‚Äî brak przewagi.
    H < 0.45: mean-reversion ‚Äî arbitra≈º statystyczny.
    Odwo≈Çanie: Hurst (1951), Peters (1994), Lo (1991).
    """
    returns = np.log(price_series / price_series.shift(1)).dropna().values
    n = len(returns)
    if n < max_lag * 2:
        max_lag = n // 4

    lags = range(min_lag, max(min_lag + 1, max_lag))
    rs_means = []
    valid_lags = []

    for lag in lags:
        chunks = [returns[i : i + lag] for i in range(0, n - lag, lag)]
        rs_chunk = []
        for chunk in chunks:
            adj = chunk - chunk.mean()
            cs  = np.cumsum(adj)
            r   = cs.max() - cs.min()
            s   = chunk.std(ddof=1)
            if s > 0:
                rs_chunk.append(r / s)
        if rs_chunk:
            rs_means.append(np.mean(rs_chunk))
            valid_lags.append(lag)

    if len(valid_lags) < 5:
        return 0.5

    log_lags = np.log(valid_lags)
    log_rs   = np.log(rs_means)
    H, _     = np.polyfit(log_lags, log_rs, 1)
    return float(np.clip(H, 0.0, 1.0))


# ‚îÄ‚îÄ‚îÄ Omega Ratio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def calculate_omega_ratio(returns: np.ndarray, threshold: float = 0.0) -> float:
    """
    Omega Ratio (Shadwick & Keating 2002).
    Nie zak≈Çada normalno≈õci ‚Äî idealna miara dla aktyw√≥w z grubymi ogonami.
    Omega > 1.0 = aktywo zarabia wiƒôcej ni≈º traci (Risky Sleeve worthy).
    """
    gains = np.maximum(returns - threshold, 0)
    losses = np.maximum(threshold - returns, 0)
    total_gains  = gains.sum()
    total_losses = losses.sum()
    if total_losses < 1e-10:
        return 999.0
    return float(total_gains / total_losses)


# ‚îÄ‚îÄ‚îÄ Amihud Illiquidity ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def calculate_amihud_ratio(price_series: pd.Series, volume_series: pd.Series,
                           lookback: int = 252) -> float:
    """
    Amihud (2002) Illiquidity Ratio.
    lower = bardziej p≈Çynne = lepsze do Risky Sleeve (ma≈Çe koszty transakcyjne).
    """
    ret = np.abs(np.log(price_series / price_series.shift(1)).dropna())
    dollar_vol = (price_series * volume_series).dropna()
    common_idx = ret.index.intersection(dollar_vol.index)
    if len(common_idx) < 20:
        return np.nan
    ret_     = ret.loc[common_idx]
    dvol_    = dollar_vol.loc[common_idx]
    ratio    = (ret_ / dvol_.replace(0, np.nan)).dropna()
    return float(ratio.tail(lookback).mean() * 1e6)  # Skalujemy do czytelnej liczby


# ‚îÄ‚îÄ‚îÄ Price Momentum (12-1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def calculate_momentum_12_1(price_series: pd.Series) -> float:
    """
    12-1 Momentum Factor (Jegadeesh & Titman 1993).
    Zwrot z ostatnich 12M pomniejszony o ostatni miesiƒÖc (skip=21 dni).
    Silny momentum ‚Üí aktywo trenduje ‚Üí wy≈ºszy Score Barbella.
    """
    if len(price_series) < 273:  # 252 + 21
        if len(price_series) < 63:
            return 0.0
        # Kr√≥tka wersja: 3M momentum
        return float(price_series.iloc[-1] / price_series.iloc[-63] - 1)

    skip  = 21
    start = -(252 + skip)
    end   = -skip
    return float(price_series.iloc[-1 - skip] / price_series.iloc[start] - 1)


# ‚îÄ‚îÄ‚îÄ Metryki Wypuk≈Ço≈õci ‚Äî G≈Ç√≥wna Funkcja ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def calculate_convecity_metrics(ticker: str, price_series: pd.Series,
                                volume_series: pd.Series | None = None) -> dict | None:
    """
    Oblicza pe≈Çen zestaw metryk dla skanera WYPUK≈ÅO≈öCI Barbella.
    Cel: znale≈∫ƒá aktywa do RISKY SLEEVE ‚Äî maksymalna asymetria zysk/strata.
    """
    if len(price_series) < 50:
        return None

    returns = np.log(price_series / price_series.shift(1)).dropna().values

    if len(returns) < 30:
        return None

    # 1. Podstawowe statystyki roczne
    vol_ann  = float(np.std(returns) * np.sqrt(252))
    mean_ann = float(np.mean(returns) * 252)

    # 2. Wy≈ºsze momenty
    skew_val = float(skew(returns))
    kurt_val = float(kurtosis(returns))  # Excess kurtosis (Fisher)

    # 3. Ratios hedgefundowe
    sharpe  = calculate_sharpe(returns)
    sortino = calculate_sortino(returns)
    max_dd  = calculate_max_drawdown(price_series)

    # 4. EVT ‚Äî Prawy ogon (szukamy wypuk≈Ço≈õci)
    xi_right = evt_pot_right_tail(returns)

    # 5. EVT ‚Äî Lewy ogon (Crash Risk) ‚Äî niski = bezpieczniejszy
    xi_left  = evt_pot_left_tail(returns)

    # 6. Omega Ratio (nie zak≈Çada normalno≈õci)
    omega = calculate_omega_ratio(returns)

    # 7. Hurst Exponent (trend vs. mean-reversion)
    hurst = calculate_hurst_exponent(price_series)

    # 8. Momentum 12-1
    momentum = calculate_momentum_12_1(price_series)

    # 9. Amihud Ratio (opcjonalne ‚Äî wymaga wolumenu)
    amihud = np.nan
    if volume_series is not None and len(volume_series) > 50:
        amihud = calculate_amihud_ratio(price_series, volume_series)

    # 10. Kelly (Fat-Tail Safe) ‚Äî po≈Çowa Kelly jako bezpieczna dawka
    risk_free = 0.04
    if vol_ann > 0 and not np.isnan(vol_ann):
        kelly_full = (mean_ann - risk_free) / (vol_ann ** 2)
        kelly_safe = kelly_full * 0.5  # Shrinkage 50%
    else:
        kelly_full = kelly_safe = 0.0

    # 11. Variance Drag (koszt zmienno≈õci)
    var_drag = 0.5 * (vol_ann ** 2)

    return {
        "Ticker":             ticker,
        "Annual Return":      mean_ann,
        "Volatility":         vol_ann,
        "Skewness":           skew_val,
        "Kurtosis":           kurt_val,
        "EVT Shape (Tail)":   xi_right,   # Prawy ogon (zyski) ‚Äî wysoki = wypuk≈Çe
        "EVT Left Tail":      xi_left,    # Lewy ogon (straty) ‚Äî niski = bezpieczne
        "Omega":              omega,
        "Hurst":              hurst,
        "Momentum_1Y":        momentum,
        "Amihud":             amihud,
        "Sharpe":             sharpe,
        "Sortino":            sortino,
        "Max Drawdown":       max_dd,
        "Variance Drag":      var_drag,
        "Kelly Full":         kelly_full,
        "Kelly Safe (50%)":   kelly_safe,
    }


# ‚îÄ‚îÄ‚îÄ Composite Barbell Score (Wa≈ºony Z-Score) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def score_asset_composite(metrics_df: pd.DataFrame) -> pd.Series:
    """
    Composite Barbell Score ‚Äî wa≈ºony Z-Score 7 czynnik√≥w.
    Metodologia: AQR Multi-Factor Composite (Asness, Frazzini, Pedersen 2012).
    
    Czynniki POSITIVE (wysoki ‚Üí lepszy kandydat do Risky Sleeve):
      EVT Right Tail, Skewness, Omega, Hurst, Momentum
    Czynniki NEGATIVE (wysoki ‚Üí gorszy, karujemy):
      EVT Left Tail (Crash Risk), Amihud (Illiquidity)
    """
    factor_weights = {
        "EVT Shape (Tail)": +0.30,   # G≈Ç√≥wny cel: wypuk≈Ço≈õƒá prawego ogona
        "Skewness":         +0.20,   # Asymetria zysk/strata (Taleb)
        "Omega":            +0.20,   # Gain/Loss ratio (bez za≈Ço≈ºenia normalno≈õci)
        "Momentum_1Y":      +0.15,   # Czynnik trendu (JT 1993)
        "Hurst":            +0.10,   # Persistencja (Peters 1994)
        "EVT Left Tail":    -0.20,   # Crash Risk ‚Äî KARA za gruby lewy ogon
        "Amihud":           -0.10,   # Illiquidity ‚Äî KARA za brak p≈Çynno≈õci
    }

    composite = pd.Series(0.0, index=metrics_df.index)

    for col, weight in factor_weights.items():
        if col not in metrics_df.columns:
            continue
        series = metrics_df[col].copy()
        # Pomi≈Ñ kolumny z samymi NaN
        if series.isna().all():
            continue
        mu  = series.mean(skipna=True)
        std = series.std(skipna=True)
        if std < 1e-9:
            continue
        zscore = (series - mu) / std
        composite += weight * zscore.fillna(0)

    return composite


def score_asset(metrics: dict) -> float:
    """
    Legacy single-asset score (u≈ºywany dla kompatybilno≈õci z pipeline V1).
    UWAGA: Nale≈ºy u≈ºywaƒá score_asset_composite() dla pe≈Çnej analiz df.
    """
    if metrics is None:
        return -999.0

    score = 0.0

    # EVT Prawy Ogon (Zyski)
    xi_right = metrics.get("EVT Shape (Tail)", np.nan)
    if not np.isnan(xi_right):
        if xi_right > 0.4:
            score += 60
        elif xi_right > 0.2:
            score += 30
        elif xi_right > 0:
            score += 10

    # EVT Lewy Ogon (Crash Risk) ‚Äî KARA
    xi_left = metrics.get("EVT Left Tail", np.nan)
    if not np.isnan(xi_left):
        if xi_left > 0.6:
            score -= 50  # Wysokie ryzyko krachu ‚Äî dyskwalifikacja
        elif xi_left > 0.3:
            score -= 20

    # Sko≈õno≈õƒá (musi byƒá +)
    skew_val = metrics.get("Skewness", 0.0)
    if skew_val > 0:
        score += 25 * skew_val
    else:
        score -= 40  # Ujemna skewness = ryzyko lewego ogona

    # Omega
    omega = metrics.get("Omega", 1.0)
    if omega is not None and not np.isnan(omega):
        if omega > 1.5:
            score += 25
        elif omega > 1.0:
            score += 10
        else:
            score -= 15

    # Momentum
    mom = metrics.get("Momentum_1Y", 0.0)
    if mom is not None and not np.isnan(mom):
        if mom > 0.20:
            score += 15
        elif mom > 0:
            score += 5
        else:
            score -= 10

    # Kelly (aktywo musi zarabiaƒá)
    kelly = metrics.get("Kelly Full", 0.0)
    if kelly > 0.1:
        score += 15
    elif kelly <= 0:
        score -= 25

    return float(score)


# ‚îÄ‚îÄ‚îÄ Hierarchical Dendrogram (HRP) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def compute_hierarchical_dendrogram(returns_df: pd.DataFrame):
    """
    HRP Dendrogram ‚Äî Lopez de Prado (2016).
    Zastƒôpuje MST wizualizacjƒÖ zagnie≈ºd≈ºonej struktury ryzyka.
    """
    tickers = returns_df.columns.tolist()
    if len(tickers) < 2:
        return None

    corr = returns_df.corr().fillna(0)
    dist = np.sqrt(np.clip(0.5 * (1 - corr), 0, 1))
    condensed = ssd.squareform(dist.values, checks=False)
    Z = sch.linkage(condensed, method="ward")

    fig = ff.create_dendrogram(
        dist.values,
        labels=tickers,
        linkagefun=lambda x: sch.linkage(x, method="ward"),
        color_threshold=float(np.percentile(Z[:, 2], 70)),
    )
    fig.update_layout(
        title="üå≥ Dendrogram Hierarchiczny (HRP ‚Äî Lopez de Prado 2016)",
        template="plotly_dark",
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(15,15,25,0.9)",
        height=500,
        xaxis_title="Zgrupowane Aktywa",
        yaxis_title="Dystans Korelacyjny",
        font=dict(family="Inter", color="white"),
    )
    fig.update_xaxes(showspikes=True, spikecolor="white", spikethickness=1, spikedash="dot", spikemode="across")
    fig.update_yaxes(showspikes=True, spikecolor="white", spikethickness=1, spikedash="dot", spikemode="across")
    return fig


# ‚îÄ‚îÄ‚îÄ MST Correlation Network (Mantegna 1999) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def compute_correlation_network(returns_df: pd.DataFrame, metrics_df: pd.DataFrame = None):
    """
    Minimum Spanning Tree (Mantegna 1999) ‚Äî Sieƒá korelacji.
    """
    if not HAS_NETWORKX:
        return None

    tickers = returns_df.columns.tolist()
    if len(tickers) < 2:
        return None

    corr = returns_df.corr()
    dist = np.sqrt(2 * (1 - corr))

    G = nx.Graph()
    for i, t1 in enumerate(tickers):
        for j, t2 in enumerate(tickers):
            if i < j:
                G.add_edge(t1, t2, weight=float(dist.loc[t1, t2]),
                           corr=float(corr.loc[t1, t2]))

    mst = nx.minimum_spanning_tree(G, weight="weight")
    pos = nx.spring_layout(mst, seed=42, k=2.0)

    if metrics_df is not None and "Barbell Score" in metrics_df.columns:
        score_map    = metrics_df.set_index("Ticker")["Barbell Score"].to_dict() \
                       if "Ticker" in metrics_df.columns else {}
        node_colors  = [score_map.get(t, 0.0) for t in mst.nodes()]
        color_label  = "Barbell Score"
    elif metrics_df is not None and "Sharpe" in metrics_df.columns:
        sharpe_map   = metrics_df.set_index("Ticker")["Sharpe"].to_dict() \
                       if "Ticker" in metrics_df.columns else {}
        node_colors  = [sharpe_map.get(t, 0.0) for t in mst.nodes()]
        color_label  = "Sharpe Ratio"
    else:
        node_colors  = [dict(mst.degree())[t] for t in mst.nodes()]
        color_label  = "Degree"

    edge_x, edge_y = [], []
    for u, v, _ in mst.edges(data=True):
        x0, y0 = pos[u]
        x1, y1 = pos[v]
        edge_x += [x0, x1, None]
        edge_y += [y0, y1, None]

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=1.5, color="rgba(150,150,200,0.5)"),
        hoverinfo="none", mode="lines", name="Korelacja (MST)",
    )

    node_x = [pos[t][0] for t in mst.nodes()]
    node_y = [pos[t][1] for t in mst.nodes()]
    labels = list(mst.nodes())

    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode="markers+text",
        text=labels, textposition="top center",
        textfont=dict(color="white", size=11),
        marker=dict(
            size=20, color=node_colors,
            colorscale="RdYlGn",
            colorbar=dict(title=color_label, thickness=12),
            showscale=True, line=dict(color="white", width=1.5),
        ),
        hovertemplate=[
            f"<b>{t}</b><br>{color_label}: {c:.2f}<extra></extra>"
            for t, c in zip(labels, node_colors)
        ],
        name="Aktywa",
    )

    fig = go.Figure([edge_trace, node_trace])
    fig.update_layout(
        title="üï∏Ô∏è Sieƒá Korelacji MST (Mantegna 1999)",
        showlegend=False, hovermode="closest",
        template="plotly_dark",
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(15,15,25,0.9)",
        height=500,
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        font=dict(family="Inter", color="white"),
        margin=dict(l=20, r=20, t=50, b=20),
    )
    fig.update_xaxes(showspikes=True, spikecolor="white", spikethickness=1, spikedash="dot", spikemode="across")
    fig.update_yaxes(showspikes=True, spikecolor="white", spikethickness=1, spikedash="dot", spikemode="across")
    return fig
